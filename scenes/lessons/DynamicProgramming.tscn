[gd_scene load_steps=13 format=3 uid="uid://hxtk5a3xqhil"]

[ext_resource type="Script" path="res://scenes/scripts/Lesson.gd" id="1_lfwre"]
[ext_resource type="Theme" uid="uid://dhymcf16h2rcx" path="res://resources/themes/impact_theme.tres" id="2_8x7pl"]
[ext_resource type="Theme" uid="uid://boy78iwy5tcum" path="res://resources/themes/lesson_theme.tres" id="3_amlk2"]
[ext_resource type="Texture2D" uid="uid://chrs2afaa7hkd" path="res://resources/lessons/images/BellmanQ.png" id="4_u48eo"]
[ext_resource type="Texture2D" uid="uid://bs0u15qf4rqhu" path="res://resources/lessons/images/dp1.png" id="5_kdsl7"]
[ext_resource type="Theme" uid="uid://dpvfhrd6522wp" path="res://resources/themes/main_theme.tres" id="5_mpgrp"]
[ext_resource type="FontFile" uid="uid://baq7wadp53op4" path="res://resources/fonts/Ranchers-v1/Ranchers-Regular.ttf" id="6_j3jci"]
[ext_resource type="Texture2D" uid="uid://cal43xlqsi7v2" path="res://resources/lessons/images/dp2.png" id="6_vils2"]
[ext_resource type="Script" path="res://addons/jam_essentials/scenes/scripts/TransitionButton.gd" id="7_4jv8i"]
[ext_resource type="Texture2D" uid="uid://dv38rudc4wdvo" path="res://resources/lessons/images/dp3.png" id="7_y7bfi"]
[ext_resource type="Texture2D" uid="uid://doysk1ho75c5" path="res://resources/lessons/images/learners.png" id="8_olt6l"]
[ext_resource type="Texture2D" uid="uid://beasjojm5ldmx" path="res://resources/lessons/images/values.png" id="9_03v0q"]

[node name="Lesson" type="Polygon2D"]
position = Vector2(528, 0)
polygon = PackedVector2Array(448, -16, 480, 560, 976, 560, 976, -16)
script = ExtResource("1_lfwre")

[node name="LessonShadow" type="Polygon2D" parent="."]
show_behind_parent = true
position = Vector2(-7, 0)
color = Color(0, 0, 0, 1)
polygon = PackedVector2Array(448, -16, 480, 560, 976, 560, 976, -16)

[node name="Lessons" type="ScrollContainer" parent="."]
clip_contents = false
offset_left = 512.0
offset_top = 16.0
offset_right = 944.0
offset_bottom = 528.0
theme = ExtResource("5_mpgrp")

[node name="VBox" type="VBoxContainer" parent="Lessons"]
custom_minimum_size = Vector2(424, 0)
layout_mode = 2
alignment = 1

[node name="Title" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
theme = ExtResource("2_8x7pl")
theme_override_font_sizes/normal_font_size = 32
bbcode_enabled = true
text = "[center]Dynamic Programming"
fit_content = true

[node name="1" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "From the last lesson, we know that if we know the expected reward for each state - or, in other words, if we know the environment's value function - we can easily derive an efficient policy from that. For instance, if we know that going up will wield an expected future reward of 2.0 while going down will wield an expected future reward of -1.0, we should choose to go up. If we know that for every state, we then know how to act in every state.

The first approach we are going to study for solving that problem is based on one of the Bellman expectation equations, shown below."
fit_content = true

[node name="2" type="TextureRect" parent="Lessons/VBox"]
layout_mode = 2
texture = ExtResource("4_u48eo")
stretch_mode = 3

[node name="3" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "This may look a bit daunting at first, so let's unpack it.

- Q is our action value function. This means Q(s, a) is the expected future reward of taking action a at state s. This is what we're looking for!
- P is the environment's state transition function. It models the probability of each outcome for each action in each state, as in the stochastic environments we saw in the environments lesson
- R is the reward we just got
- That greek letter, gamma, is our discount factor

Essentially, what this equation tells us is that the value of taking action a in each state depends on the resulting reward R as well as the expected future reward for the state we just landed in!

Consider, for instance, that after taking a particular action, we gained a reward of -0.2 and we landed in a state with an expected future reward of 1.0. It stands, then, that the expected future reward of that state should be -0.2 + 1.0 = 0.8.

In practice, we weight those values for the probability of landing in a particular state, to account for the stochasticity of our environment.

Furthermore, we weight the expected future reward of our resulting state by our discount factor gamma. Effectively, gamma tells us how much we should care about future reward. [wave]If gamma is 1.0, future reward is as important as immediate reward, and if gamma is 0.0, future reward doesn't matter at all[/wave].

Applying this equation to all state and actions, we get a system in which all states end up depending on each other, which we can solve with many different methods.

The only issue for applying this method in practice is that our agent doesn't have access to the probability of landing in each state. [wave]It simply experiences the consequences of its actions, not the mechanics of the environment that brought it there[/wave]. The agent should, therefore, not have access to a complete model of the environment.

You can think of this as the difference between walking through a hedge maze and solving a maze with a pen and paper. In the first example, you don't know the entirety of the maze, so you have to do some exploring at random to try to find your way out. In the latter, you can see the entire maze from above, so you can consider all possibilities to plan the best route.

Reinforcement learning is focused on treating problems in which the agent doesn't know the entire environment. For our first algorithm, however, we are going to explore how to solve our problem once we know the entirety of our environment.

Our first algorithm is based on using dynamic programming to solve the system of equations for each state-action pair formed by the application of the Bellman equation. Effectively, for each state-action pair we will apply the Bellman equation to update its value through the value of the resulting state. Essentially, we are solving the value of each state-action pair through the equation we studied, which is not that complicated if we assume we know everything about the environment.

We can think of the values as spreading through the environment in each iteration. That is because, each time they update, they consider only their neighboring states.

Consider the following example, with a base and lose reward value of 0.0.
"
fit_content = true

[node name="4" type="TextureRect" parent="Lessons/VBox"]
layout_mode = 2
texture = ExtResource("5_kdsl7")
stretch_mode = 3

[node name="5" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "After the first update, only the tiles that are nearest to the goal are updated, as they are the only ones that receive direct reward."
fit_content = true

[node name="6" type="TextureRect" parent="Lessons/VBox"]
layout_mode = 2
texture = ExtResource("6_vils2")
stretch_mode = 3

[node name="8" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "On the second update, the neighboring tiles of the ones that were previously updated are able to update based on the values calculated in the previous step."
fit_content = true

[node name="7" type="TextureRect" parent="Lessons/VBox"]
layout_mode = 2
texture = ExtResource("7_y7bfi")
stretch_mode = 3

[node name="9" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "This process continues as the values of tiles spreads to its neighbors on a step by step basis.

Now, it's time to see this algorithms operating in practice. We are introducing some new tabs to our sandbox."
fit_content = true

[node name="10" type="TextureRect" parent="Lessons/VBox"]
layout_mode = 2
texture = ExtResource("8_olt6l")
stretch_mode = 3

[node name="11" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "To the right, you can access the learners tab. For now, the only options is DP (Dynamic programming!), but more algorithms will appear as they're covered here.
"
fit_content = true

[node name="12" type="TextureRect" parent="Lessons/VBox"]
layout_mode = 2
texture = ExtResource("9_03v0q")
stretch_mode = 3

[node name="13" type="RichTextLabel" parent="Lessons/VBox"]
clip_contents = false
layout_mode = 2
mouse_filter = 1
theme = ExtResource("3_amlk2")
bbcode_enabled = true
text = "The values tab will allow you to modify properties used in the learning algorithms. In the case of DP, you can change the gamma value which, again, dictates how much we care about future reward.

Now, you can play with the following experiment to see how DP works in practice. In the sandbox, we are doing a single update for every state per episode to allow for easier visualization, but keep in mind that, in practice, we could just do it all at once and instantly get the best solution for a given environment.

Here are a couple things to try out:

- Try changing the gamma value from 0.0 to 1.0. How does that change our final solution?
- Slow the simulation down and watch out for the way the value spreads from tiles with different rewards to their neighbors"
fit_content = true

[node name="Sandbox" type="Button" parent="Lessons/VBox"]
layout_mode = 2
theme = ExtResource("5_mpgrp")
theme_override_colors/font_hover_color = Color(1, 0.713726, 0, 1)
theme_override_fonts/font = ExtResource("6_j3jci")
text = "Experiment!"
flat = true
script = ExtResource("7_4jv8i")
transition_path = "res://scenes/experiments/DynamicProgramming.tscn"
